{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aggregation function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import glob # for listing files in the directory, asemejan un patrÃ³n especificado de acuerdo con las reglas de coincidencia de Unix\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device) # If gpu is available, cuda will be printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the architectures used for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convoloitional neural network\n",
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        \"\"\" \n",
    "\n",
    "        Output size after convolutional filter:\n",
    "        ((input_size - kernel + 2 * padding)/ stride) + 1\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Input shape = (3, 224, 224)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (16, 224, 224) -- Same shape as input, but more channels\n",
    "        # Add batch normalization\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=16) # Shape = (16, 224, 224)\n",
    "        # Add activation function\n",
    "        self.relu1 = nn.ReLU() # Shape = (16, 224, 224)\n",
    "        # Max pool 1, reuduce the image size be factor 2\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # Shape = (16, 112, 112)\n",
    "\n",
    "        # Add second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Shape = (32, 112, 112)\n",
    "        # Add activation function\n",
    "        self.relu2 = nn.ReLU() # Shape = (32, 112, 112)\n",
    "\n",
    "        # Add third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1) # Shape = (64, 112, 112)\n",
    "        # Add batch normalization\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=64) # Shape = (64, 112, 112)\n",
    "        # Add activation function\n",
    "        self.relu3 = nn.ReLU() # Shape = (64, 112, 112)\n",
    "\n",
    "        # Linear layer (64 * 112 * 112 -> 2)\n",
    "        self.fc = nn.Linear(in_features=64 * 112 * 112, out_features=num_classes)\n",
    "\n",
    "    # Feed forward function\n",
    "    def forward(self, input):\n",
    "        # Convolutional layer\n",
    "        output = self.conv1(input)\n",
    "        # Batch normalization\n",
    "        output = self.bn1(output)\n",
    "        # Activation function\n",
    "        output = self.relu1(output)\n",
    "        # Max pool\n",
    "        output = self.pool1(output)\n",
    "\n",
    "        # Convolutional layer\n",
    "        output = self.conv2(output)\n",
    "        # Activation function\n",
    "        output = self.relu2(output)\n",
    "\n",
    "        # Convolutional layer\n",
    "        output = self.conv3(output)\n",
    "        # Batch normalization\n",
    "        output = self.bn3(output)\n",
    "        # Activation function\n",
    "        output = self.relu3(output)\n",
    "\n",
    "        # Above output will be in matrix form with shape (64, 112, 112)\n",
    "\n",
    "        # Flatten\n",
    "        output = output.view(-1, 64 * 112 * 112)\n",
    "\n",
    "        # Linear layer (output -> 2)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderHPyloris(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoderHPyloris, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        The proposed autoencoder has 3 convolutional blocks with one convolutional layer,\n",
    "        batch normalization and leakyrelu activation each. The size of the convolutional\n",
    "        kernel is 3 and the number of neurons and stride of each layer are, respectively,\n",
    "        [32,64,64] and [1,2,2].\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encoder =  nn.Sequential(\n",
    "                        nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # Convolutional Layer 1\n",
    "                        nn.BatchNorm2d(32),  # Batch Normalization 1\n",
    "                        nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU Activation 1\n",
    "                       \n",
    "                        nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Convolutional Layer 2\n",
    "                        nn.BatchNorm2d(64),  # Batch Normalization 2\n",
    "                        nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU Activation 2\n",
    "\n",
    "                        nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # Convolutional Layer 3\n",
    "                        nn.BatchNorm2d(128),  # Batch Normalization 3\n",
    "                        nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU Activation 3\n",
    "                        \n",
    "                        nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # Convolutional Layer 3\n",
    "                        nn.BatchNorm2d(256),  # Batch Normalization 3\n",
    "                        nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU Activation 3\n",
    "                        \n",
    "                        nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),  # Convolutional Layer 3\n",
    "                        nn.BatchNorm2d(256),  # Batch Normalization 3\n",
    "                        nn.LeakyReLU(0.2, inplace=True)  # LeakyReLU Activation 3\n",
    "        )\n",
    "        \n",
    "        self.decoder =  nn.Sequential(\n",
    "                        nn.ConvTranspose2d(256, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.BatchNorm2d(256),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "                        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        \n",
    "                        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        \n",
    "                        nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        \n",
    "                        nn.ConvTranspose2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image):\n",
    "        encoded_img = self.encoder(image)\n",
    "        decoded_img = self.decoder(encoded_img)\n",
    "        return decoded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fhome/gia01/anaconda3/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/fhome/gia01/anaconda3/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "modelVGG = models.vgg16(pretrained=True) # VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_patch(model, image, transform, inverse_transform, threshold=0.25, device='cpu'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        MODEL: AutoEncoder model, given an input image, returns the reconstructed image\n",
    "        IMAGE: a 3 channel image that will be reconstructed\n",
    "        TRANSFORM: transforms that will be applied to input image\n",
    "        INVERSE_TRANSFORM: transforms that will be applied to reconstructed image\n",
    "        THRESHOLD (default=0.25): the threshold that will classify the patch\n",
    "        DEVICE (default='cpu'): device where the model will be loaded (cpu or cuda)\n",
    "    Outputs:\n",
    "        INFECTION: 0 if no infection, 1 if infection\n",
    "    \"\"\"\n",
    "\n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Apply the transformations to convert the PIL image to a PyTorch tensor\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    \n",
    "    # Move the tensor to the specified device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        #original = transform(image).unsqueeze(0)\n",
    "        #original.to(device)\n",
    "        reconstructed_image = inverse_transform(model(image_tensor).squeeze(0))\n",
    "\n",
    "        hsv_original_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2HSV)\n",
    "        red_pixels_original = np.sum(np.logical_or((hsv_original_image[:, :, 0] < 20) & (hsv_original_image[:, :, 0] >= 0),\n",
    "                                            (hsv_original_image[:, :, 0] > 160) & (hsv_original_image[:, :, 0] <= 180)))\n",
    "        percentage_original = 100 * (red_pixels_original / (224 * 224))\n",
    "        \n",
    "        hsv_reconstructed_image = cv2.cvtColor(np.array(reconstructed_image), cv2.COLOR_RGB2HSV)\n",
    "        red_pixels_reconstructed = np.sum(np.logical_or((hsv_reconstructed_image[:, :, 0] < 20) & (hsv_reconstructed_image[:, :, 0] >= 0), \n",
    "                                                        (hsv_reconstructed_image[:, :, 0] > 160) & (hsv_reconstructed_image[:, :, 0] <= 180)))\n",
    "        percentage_reconstructed = 100 * (red_pixels_reconstructed / (224 * 224))\n",
    "        \n",
    "        lost_pixels_ratio = percentage_original-percentage_reconstructed\n",
    "        \n",
    "        if lost_pixels_ratio >= threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transform = transforms.Compose([transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], \n",
    "                                                                std=[1/0.229, 1/0.224, 1/0.225]), \n",
    "                                        transforms.ToPILImage()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the train set for the ensemble method training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'INFECTED': {'B22-65': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-65_1/', 'B22-222': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-222_1/', 'B22-130': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-130_1/', 'B22-225': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-225_1/', 'B22-263': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-263_1/'}, 'NOT_INFECTED': {'B22-282': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-282_1/', 'B22-168': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-168_1/', 'B22-227': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-227_1/', 'B22-111': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-111_1/', 'B22-189': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-189_1/'}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionary to store the data\n",
    "train_dict = {'INFECTED': {}, 'NOT_INFECTED': {}}\n",
    "\n",
    "# Read the file line by line\n",
    "with open('/fhome/gia01/vl_project2/autoencoder/txt files/train_thresholds.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Strip any whitespace and split the line by comma\n",
    "        parts = line.strip().split(',')\n",
    "        # Assign the parts to variables for clarity\n",
    "        sample_id, path, status = parts[0], parts[1], int(parts[2])\n",
    "        # Depending on the status, add the entry to the respective dictionary key\n",
    "        if status == 1:\n",
    "            train_dict['INFECTED'][sample_id] = path\n",
    "        else:\n",
    "            train_dict['NOT_INFECTED'][sample_id] = path\n",
    "\n",
    "# Now data_dict is populated with the data in the format you wanted.\n",
    "print(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the test set for the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'INFECTED': {'B22-272': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-272_1/', 'B22-286': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-286_1/', 'B22-293': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-293_1/', 'B22-294': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-294_1/', 'B22-295': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-295_1/'}, 'NOT_INFECTED': {'B22-28': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-28_1/', 'B22-34': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-34_1/', 'B22-53': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-53_1/', 'B22-87': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-87_1/', 'B22-112': '/fhome/mapsiv/QuironHelico/CroppedPatches/B22-112_1/'}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionary to store the data\n",
    "test_dict = {'INFECTED': {}, 'NOT_INFECTED': {}}\n",
    "\n",
    "# Read the file line by line\n",
    "with open('/fhome/gia01/vl_project2/autoencoder/txt files/test_aggregate.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Strip any whitespace and split the line by comma\n",
    "        parts = line.strip().split(',')\n",
    "        # Assign the parts to variables for clarity\n",
    "        sample_id, path, status = parts[0], parts[1], int(parts[2])\n",
    "        # Depending on the status, add the entry to the respective dictionary key\n",
    "        if status == 1:\n",
    "            test_dict['INFECTED'][sample_id] = path\n",
    "        else:\n",
    "            test_dict['NOT_INFECTED'][sample_id] = path\n",
    "\n",
    "# Now data_dict is populated with the data in the format you wanted.\n",
    "print(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load the three models used for the diagnosis: CNN, AE, VGG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the checkpoint for the CNN model\n",
    "checkpointCNN = torch.load(\"best_checkpoint1.model\")\n",
    "\n",
    "# Instantiate the CNN model and load the checkpoint into it\n",
    "modelCNN = ConvNet(num_classes = 2)\n",
    "modelCNN.load_state_dict(checkpointCNN)\n",
    "modelCNN.to(device)\n",
    "modelCNN.eval()\n",
    "\n",
    "model_path = '/fhome/gia01/vl_project2/autoencoder/model_pkl/10_EPOCH_MSE_AE.pkl'\n",
    "with open(model_path, 'rb') as weights:\n",
    "    state = pickle.load(weights)\n",
    "    modelAE = AutoEncoderHPyloris()\n",
    "    modelAE.load_state_dict(state)\n",
    "modelAE.to(device)    \n",
    "\n",
    "checkpoint = torch.load(\"best_checkpoint.modelVGG\") # Load checkpoint\n",
    "\n",
    "modelVGG = models.vgg16(pretrained=True)\n",
    "modelVGG.classifier[6] = torch.nn.Linear(in_features=4096, out_features=2)\n",
    "modelVGG.load_state_dict(checkpoint) # Load checkpoint into the model\n",
    "modelVGG.to(device)\n",
    "modelVGG.eval() # Set model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation transforms\n",
    "myTransforms = transforms.Compose([ transforms.Resize((224,224)), # Resize the image to 224x224\n",
    "                                    transforms.ToTensor(),       # Convert the image to a tensor\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize the image\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_threshold = 3 # CNN threshold\n",
    "AE_threshold = 7 # AE threshold\n",
    "VGG_threshold = 7 # VGG threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(img_path, myTransforms, model):\n",
    "    # Load the image\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    # Convert RGBA to RGB if necessary\n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Apply the transformations\n",
    "    image_tensor = myTransforms(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move the model and tensor to the same device\n",
    "    device = next(model.parameters()).device  # Get the device from the model\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Forward pass through the model to get the predictions\n",
    "    output = model(image_tensor)\n",
    "\n",
    "    # Move the output back to CPU and convert to numpy array for further processing\n",
    "    output = output.cpu().data.numpy()\n",
    "\n",
    "    # Get the index of the highest probability class\n",
    "    index = output.argmax()\n",
    "\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaration of the function in order to make the conversion\n",
    "def calculate_confidence(percentage, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the confidence level based on a given percentage using a rule of three.\n",
    "\n",
    "    Parameters:\n",
    "    percentage (float): A percentage value where 0.1 maps to a confidence level of 1.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated confidence level, capped at 1 for percentages over 0.1.\n",
    "    \"\"\"\n",
    "    # If the percentage is greater than 0.1, the confidence is 1\n",
    "    if percentage >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        # Otherwise, scale the confidence linearly from 0 to 1 as the percentage goes from 0 to 0.1\n",
    "        # Using the rule of three: percentage / 0.1 = confidence / 1\n",
    "        confidence = percentage / threshold\n",
    "        return confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 1: for CNN  | 1 - 1\n",
      "Patient 1: for AE   | 1 - 0.038\n",
      "Patient 1: for VGG  | 1 - 1\n",
      "\n",
      "Patient 2: for CNN  | 1 - 1\n",
      "Patient 2: for AE   | 1 - 0.0\n",
      "Patient 2: for VGG  | 1 - 1\n",
      "\n",
      "Patient 3: for CNN  | 1 - 1\n",
      "Patient 3: for AE   | 1 - 0.03\n",
      "Patient 3: for VGG  | 1 - 1\n",
      "\n",
      "Patient 4: for CNN  | 1 - 1\n",
      "Patient 4: for AE   | 1 - 0.015\n",
      "Patient 4: for VGG  | 1 - 1\n",
      "\n",
      "Patient 5: for CNN  | 1 - 1\n",
      "Patient 5: for AE   | 1 - 0.0\n",
      "Patient 5: for VGG  | 1 - 1\n",
      "\n",
      "Patient 6: for CNN  | 0 - 0.055\n",
      "Patient 6: for AE   | 0 - 0.0\n",
      "Patient 6: for VGG  | 0 - 0.213\n",
      "\n",
      "Patient 7: for CNN  | 0 - 0.325\n",
      "Patient 7: for AE   | 0 - 0.0\n",
      "Patient 7: for VGG  | 0 - 0.41\n",
      "\n",
      "Patient 8: for CNN  | 0 - 0.07\n",
      "Patient 8: for AE   | 0 - 0.0\n",
      "Patient 8: for VGG  | 0 - 0.127\n",
      "\n",
      "Patient 9: for CNN  | 0 - 0.659\n",
      "Patient 9: for AE   | 0 - 0.0\n",
      "Patient 9: for VGG  | 0 - 0.242\n",
      "\n",
      "Patient 10: for CNN  | 0 - 0.221\n",
      "Patient 10: for AE   | 0 - 0.0\n",
      "Patient 10: for VGG  | 0 - 0.261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_scoresCNN = list() # Store the probability estimates of being from the infected class, threshold: %3\n",
    "y_scoresAE = list() # Store the probability estimates of being from the infected class, threshold: %int()\n",
    "y_scoresVGG = list() # Store the probability estimates of being from the infected class, threshold: %int()\n",
    "\n",
    "infectedPer_listCNN = list() # Store the infected percentage for each patient\n",
    "infectedPer_listAE = list() # Store the infected percentage for each patient\n",
    "infectedPer_listVGG = list() # Store the infected percentage for each patient\n",
    "\n",
    "y_test = list() # Store the ground truth from the data, infected or not infected\n",
    "\n",
    "i = 0 # Counter\n",
    "\n",
    "# Iterate over the test set\n",
    "for label in train_dict:\n",
    "\n",
    "    patients_dict = train_dict[label] # Get the dictionary of the patients\n",
    "\n",
    "    # Determine ground truth\n",
    "    if (label == \"INFECTED\"): # Infected\n",
    "        gt = 1\n",
    "    \n",
    "    else: # Not infected\n",
    "        gt = 0\n",
    "\n",
    "    # Iterate over the patients\n",
    "    for patient in patients_dict:\n",
    "\n",
    "        patient_predictionsCNN = list() # Initialize the data structure in which the prediction for a particular patient will be stored\n",
    "        patient_predictionsAE = list() # Initialize the data structure in which the prediction for a particular patient will be stored\n",
    "        patient_predictionsVGG = list() # Initialize the data structure in which the prediction for a particular patient will be stored\n",
    "        # Get a list of all the full paths for the images from the patient\n",
    "        images_files_full_paths = [os.path.join(patients_dict[patient], file) for file in os.listdir(patients_dict[patient])]\n",
    "        \n",
    "        # Iterate over all the images of the patient\n",
    "        for image_path in images_files_full_paths:\n",
    "\n",
    "            predCNN = prediction(image_path, myTransforms, modelCNN) # Get the prediction for the patch\n",
    "            patient_predictionsCNN.append(predCNN) # Append the prediction\n",
    "\n",
    "            imageAE = Image.open(image_path) # Open image with PIL\n",
    "            predAE = predict_patch(modelAE, imageAE, transform = myTransforms, inverse_transform = inverse_transform, threshold=0.25, device='cpu') # Get the prediction for the patch\n",
    "            patient_predictionsAE.append(predAE) # Append the prediction\n",
    "\n",
    "            predVGG = prediction(image_path, myTransforms, modelVGG) # Get the prediction for the patch\n",
    "            patient_predictionsVGG.append(predVGG) # Append the prediction\n",
    "\n",
    "        value = 1 # Infected class\n",
    "\n",
    "        # Calculate the percentage of infected detected patches for a particular patient\n",
    "        infected_percentageCNN = patient_predictionsCNN.count(value) / len(patient_predictionsCNN)\n",
    "        infectedPer_listCNN.append(infected_percentageCNN)\n",
    "\n",
    "        # Calculate the percentage of infected detected patches for a particular patient\n",
    "        infected_percentageAE = patient_predictionsAE.count(value) / len(patient_predictionsAE)\n",
    "        infectedPer_listAE.append(infected_percentageAE)\n",
    "\n",
    "        # Calculate the percentage of infected detected patches for a particular patient\n",
    "        infected_percentageVGG = patient_predictionsVGG.count(value) / len(patient_predictionsVGG)\n",
    "        infectedPer_listVGG.append(infected_percentageVGG)\n",
    "\n",
    "        # Calculate the estimated probability of the model based on predefined criterion for the diagnosis\n",
    "        # += threshold of infected patches in a patient - estimated probability of infection: 100%\n",
    "        # 0% of infected patches in a patient - estimated probability of infection: 0%\n",
    "        y_scoreCNN = calculate_confidence(infected_percentageCNN, 0.03) # Threshold: %3\n",
    "        y_scoreAE = calculate_confidence(infected_percentageAE, 0.07) # Threshold: %\n",
    "        y_scoreVGG = calculate_confidence(infected_percentageVGG, 0.07) # Threshold: %\n",
    "        \n",
    "        \n",
    "        # Update y_score and y_test lists\n",
    "        y_scoresCNN.append(y_scoreCNN)\n",
    "        y_scoresAE.append(y_scoreAE)\n",
    "        y_scoresVGG.append(y_scoreVGG)\n",
    "\n",
    "        y_test.append(gt) # Ground truth\n",
    "\n",
    "        i += 1\n",
    "        print(\"Patient\", str(i) + \":\", \"for CNN  |\", gt, \"-\", round(y_scoreCNN, 3))\n",
    "        print(\"Patient\", str(i) + \":\", \"for AE   |\", gt, \"-\", round(y_scoreAE, 3))\n",
    "        print(\"Patient\", str(i) + \":\", \"for VGG  |\", gt, \"-\", round(y_scoreVGG, 3))\n",
    "        print()\n",
    "        \n",
    "        # print(y_scores) # Display scores\n",
    "        # print(y_test) # Display ground truth\n",
    "\n",
    "        # Update control data structure\n",
    "        # CNN_predictions.append((str(patient), [gt, y_score]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labelsCNN = [1 if prob >= 0.5 else 0 for prob in y_scoresCNN] # Threshold: %3\n",
    "binary_labelsAE = [1 if prob >= 0.5 else 0 for prob in y_scoresAE] # Threshold: %int()\n",
    "binary_labelsVGG = [1 if prob >= 0.5 else 0 for prob in y_scoresVGG] # Threshold: %int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you have binary_labels0025, binary_labels0035, binary_labels004, binary_labels005, etc. already defined\n",
    "# Define the thresholds for each binary label\n",
    "thresholds = ['%3', \"%7\", \"%7\"]\n",
    "# Store all the binary labels in a list\n",
    "binary_labels_list = [binary_labelsCNN, binary_labelsAE, binary_labelsVGG]\n",
    "# In order to store the true negative, false positive, false negative and true positive\n",
    "ravels = list()\n",
    "\n",
    "# Set the number of subplots based on the number of thresholds\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(thresholds) / n_cols))\n",
    "\n",
    "# Create a figure to hold all the subplots\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through the thresholds and plot each confusion matrix\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, binary_labels_list[i])\n",
    "    # Unpack the confusion matrix into true negative, false positive, false negative and true positive\n",
    "    true_neg, false_pos, false_neg, true_pos = cm.ravel()\n",
    "    # Store the ravel in a list\n",
    "    ravel = [true_neg, false_pos, false_neg, true_pos]\n",
    "    ravels.append(ravel)\n",
    "    \n",
    "    # Initialize the display object\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"0\", \"1\"])\n",
    "\n",
    "    # Plot in the next subplot\n",
    "    disp.plot(ax=axes[i], cmap='Blues', colorbar=False)\n",
    "    axes[i].set_title(f'Confusion matrix (Threshold: {threshold})')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for ax in axes[len(thresholds):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add more space between the plots\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked generalization\n",
    "\n",
    "Ensemble learning technique that uses a meta-learner to find the optimal combination of the base learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [modelCNN, modelAE, modelVGG] # List of all the methods used for the ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the predictions to create a new dataset for the meta-learner\n",
    "stacked_predictions = np.column_stack((binary_labelsCNN, binary_labelsAE, binary_labelsVGG))\n",
    "\n",
    "# Train the meta-learner on the stacked predictions\n",
    "meta_learner = LogisticRegression().fit(stacked_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for CNN model predictions: 0.9086479260708061\n",
      "Weights for AE model predictions: 0.0\n",
      "Weights for VGG model predictions: 1.3304189826002182\n"
     ]
    }
   ],
   "source": [
    "# After fitting the meta-learner\n",
    "weights = meta_learner.coef_\n",
    "\n",
    "# Print the weights for each model\n",
    "print(f'Weights for CNN model predictions: {weights[0][0]}')\n",
    "print(f'Weights for AE model predictions: {weights[0][1]}')\n",
    "print(f'Weights for VGG model predictions: {weights[0][2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 1: for CNN  | 1 - 1\n",
      "Patient 1: for AE   | 1 - 0.0\n",
      "Patient 1: for VGG  | 1 - 1\n",
      "\n",
      "Patient 2: for CNN  | 1 - 0.243\n",
      "Patient 2: for AE   | 1 - 0.0\n",
      "Patient 2: for VGG  | 1 - 0.298\n",
      "\n",
      "Patient 3: for CNN  | 1 - 1\n",
      "Patient 3: for AE   | 1 - 0.0\n",
      "Patient 3: for VGG  | 1 - 1\n",
      "\n",
      "Patient 4: for CNN  | 1 - 1\n",
      "Patient 4: for AE   | 1 - 0.0\n",
      "Patient 4: for VGG  | 1 - 1\n",
      "\n",
      "Patient 5: for CNN  | 1 - 1\n",
      "Patient 5: for AE   | 1 - 0.0\n",
      "Patient 5: for VGG  | 1 - 1\n",
      "\n",
      "Patient 6: for CNN  | 0 - 0.369\n",
      "Patient 6: for AE   | 0 - 0.0\n",
      "Patient 6: for VGG  | 0 - 0.519\n",
      "\n",
      "Patient 7: for CNN  | 0 - 0.031\n",
      "Patient 7: for AE   | 0 - 0.0\n",
      "Patient 7: for VGG  | 0 - 0.337\n",
      "\n",
      "Patient 8: for CNN  | 0 - 0.176\n",
      "Patient 8: for AE   | 0 - 0.0\n",
      "Patient 8: for VGG  | 0 - 0.572\n",
      "\n",
      "Patient 9: for CNN  | 0 - 0.124\n",
      "Patient 9: for AE   | 0 - 0.0\n",
      "Patient 9: for VGG  | 0 - 0.168\n",
      "\n",
      "Patient 10: for CNN  | 0 - 0.346\n",
      "Patient 10: for AE   | 0 - 1\n",
      "Patient 10: for VGG  | 0 - 0.742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_scoresCNN = list() # Store the probability estimates of being from the infected class, threshold: %3\n",
    "y_scoresAE = list() # Store the probability estimates of being from the infected class, threshold: %int()\n",
    "y_scoresVGG = list() # Store the probability estimates of being from the infected class, threshold: %int()\n",
    "\n",
    "infectedPer_listCNN = list() # Store the infected percentage for each patient\n",
    "infectedPer_listAE = list() # Store the infected percentage for each patient\n",
    "infectedPer_listVGG = list() # Store the infected percentage for each patient\n",
    "\n",
    "y_test = list() # Store the ground truth from the data, infected or not infected\n",
    "\n",
    "i = 0 # Counter\n",
    "\n",
    "# Iterate over the test set\n",
    "for label in test_dict:\n",
    "\n",
    "    patients_dict = test_dict[label] # Get the dictionary of the patients\n",
    "\n",
    "    # Determine ground truth\n",
    "    if (label == \"INFECTED\"): # Infected\n",
    "        gt = 1\n",
    "    \n",
    "    else: # Not infected\n",
    "        gt = 0\n",
    "\n",
    "    # Iterate over the patients\n",
    "    for patient in patients_dict:\n",
    "\n",
    "        patient_predictionsCNN = list() # Initialize the data structure in which the prediction for a particular patient will be stored\n",
    "        patient_predictionsAE = list() # Initialize the data structure in which the prediction for a particular patient will be stored\n",
    "        patient_predictionsVGG = list() # Initialize the data structure in which the prediction for a particular patient will be stored\n",
    "        # Get a list of all the full paths for the images from the patient\n",
    "        images_files_full_paths = [os.path.join(patients_dict[patient], file) for file in os.listdir(patients_dict[patient])]\n",
    "        \n",
    "        # Iterate over all the images of the patient\n",
    "        for image_path in images_files_full_paths:\n",
    "\n",
    "            predCNN = prediction(image_path, myTransforms, modelCNN) # Get the prediction for the patch\n",
    "            patient_predictionsCNN.append(predCNN) # Append the prediction\n",
    "\n",
    "            imageAE = Image.open(image_path) # Open image with PIL\n",
    "            predAE = predict_patch(modelAE, imageAE, transform = myTransforms, inverse_transform = inverse_transform, threshold=0.25, device='cpu') # Get the prediction for the patch\n",
    "            patient_predictionsAE.append(predAE) # Append the prediction\n",
    "\n",
    "            predVGG = prediction(image_path, myTransforms, modelVGG) # Get the prediction for the patch\n",
    "            patient_predictionsVGG.append(predVGG) # Append the prediction\n",
    "\n",
    "        value = 1 # Infected class\n",
    "\n",
    "        # Calculate the percentage of infected detected patches for a particular patient\n",
    "        infected_percentageCNN = patient_predictionsCNN.count(value) / len(patient_predictionsCNN)\n",
    "        infectedPer_listCNN.append(infected_percentageCNN)\n",
    "\n",
    "        # Calculate the percentage of infected detected patches for a particular patient\n",
    "        infected_percentageAE = patient_predictionsAE.count(value) / len(patient_predictionsAE)\n",
    "        infectedPer_listAE.append(infected_percentageAE)\n",
    "\n",
    "        # Calculate the percentage of infected detected patches for a particular patient\n",
    "        infected_percentageVGG = patient_predictionsVGG.count(value) / len(patient_predictionsVGG)\n",
    "        infectedPer_listVGG.append(infected_percentageVGG)\n",
    "\n",
    "        # Calculate the estimated probability of the model based on predefined criterion for the diagnosis\n",
    "        # += threshold of infected patches in a patient - estimated probability of infection: 100%\n",
    "        # 0% of infected patches in a patient - estimated probability of infection: 0%\n",
    "        y_scoreCNN = calculate_confidence(infected_percentageCNN, 0.03) # Threshold: %3\n",
    "        y_scoreAE = calculate_confidence(infected_percentageAE, 0.0007) # Threshold: %\n",
    "        y_scoreVGG = calculate_confidence(infected_percentageVGG, 0.07) # Threshold: %\n",
    "        \n",
    "        \n",
    "        # Update y_score and y_test lists\n",
    "        y_scoresCNN.append(y_scoreCNN)\n",
    "        y_scoresAE.append(y_scoreAE)\n",
    "        y_scoresVGG.append(y_scoreVGG)\n",
    "\n",
    "        y_test.append(gt) # Ground truth\n",
    "\n",
    "        i += 1\n",
    "        print(\"Patient\", str(i) + \":\", \"for CNN  |\", gt, \"-\", round(y_scoreCNN, 3))\n",
    "        print(\"Patient\", str(i) + \":\", \"for AE   |\", gt, \"-\", round(y_scoreAE, 3))\n",
    "        print(\"Patient\", str(i) + \":\", \"for VGG  |\", gt, \"-\", round(y_scoreVGG, 3))\n",
    "        print()\n",
    "        \n",
    "        # print(y_scores) # Display scores\n",
    "        # print(y_test) # Display ground truth\n",
    "\n",
    "        # Update control data structure\n",
    "        # CNN_predictions.append((str(patient), [gt, y_score]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testBinary_labelsCNN = [1 if prob >= 0.5 else 0 for prob in y_scoresCNN] # Threshold: %3\n",
    "testBinary_labelsAE = [1 if prob >= 0.5 else 0 for prob in y_scoresAE] # Threshold: %int()\n",
    "testBinary_labelsVGG = [1 if prob >= 0.5 else 0 for prob in y_scoresVGG] # Threshold: %int()\n",
    "\n",
    "# Assuming you have binary_labels0025, binary_labels0035, binary_labels004, binary_labels005, etc. already defined\n",
    "# Define the thresholds for each binary label\n",
    "thresholds = ['%3', int(), int()]\n",
    "# Store all the binary labels in a list\n",
    "binary_labels_list = [testBinary_labelsCNN, testBinary_labelsAE, testBinary_labelsVGG]\n",
    "# In order to store the true negative, false positive, false negative and true positive\n",
    "ravels = list()\n",
    "\n",
    "# Set the number of subplots based on the number of thresholds\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(thresholds) / n_cols))\n",
    "\n",
    "# Create a figure to hold all the subplots\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through the thresholds and plot each confusion matrix\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, binary_labels_list[i])\n",
    "    # Unpack the confusion matrix into true negative, false positive, false negative and true positive\n",
    "    true_neg, false_pos, false_neg, true_pos = cm.ravel()\n",
    "    # Store the ravel in a list\n",
    "    ravel = [true_neg, false_pos, false_neg, true_pos]\n",
    "    ravels.append(ravel)\n",
    "    \n",
    "    # Initialize the display object\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"0\", \"1\"])\n",
    "\n",
    "    # Plot in the next subplot\n",
    "    disp.plot(ax=axes[i], cmap='Blues', colorbar=False)\n",
    "    axes[i].set_title(f'Confusion matrix (Threshold: {threshold})')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for ax in axes[len(thresholds):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add more space between the plots\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict on the test set with the meta-learner\n",
    "test_stacked_predictions = np.column_stack((testBinary_labelsCNN, testBinary_labelsAE, testBinary_labelsVGG))\n",
    "\n",
    "# Predict on the test set with the meta-learner\n",
    "meta_predictions = meta_learner.predict(stacked_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26607615 0.73392385]\n",
      " [0.26607615 0.73392385]\n",
      " [0.26607615 0.73392385]\n",
      " [0.26607615 0.73392385]\n",
      " [0.26607615 0.73392385]\n",
      " [0.7728447  0.2271553 ]\n",
      " [0.7728447  0.2271553 ]\n",
      " [0.7728447  0.2271553 ]\n",
      " [0.57829943 0.42170057]\n",
      " [0.7728447  0.2271553 ]]\n"
     ]
    }
   ],
   "source": [
    "# Predicted probabilities for the test data\n",
    "predicted_probabilities_test = meta_learner.predict_proba(stacked_predictions)\n",
    "\n",
    "# Print the predicted probabilities for the test data\n",
    "print(predicted_probabilities_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method LogisticRegression.predict_proba of LogisticRegression()>\n"
     ]
    }
   ],
   "source": [
    "print(meta_learner.predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization in 3D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\n",
    "#Visualize en 3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataframe with the meta-learner predictions\n",
    "df = pd.DataFrame(stacked_predictions, columns=['CNN', 'AE', 'VGG'])\n",
    "# Add the ground truth\n",
    "df['Ground Truth'] = y_test\n",
    "# Add the meta-learner predictions\n",
    "df['Meta Learner'] = meta_predictions\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=3)\n",
    "# Fit the PCA and transform the data\n",
    "pca_features = pca.fit_transform(df[['CNN', 'AE', 'VGG']])\n",
    "# Add the components to the dataframe\n",
    "df['PCA1'] = pca_features[:, 0]\n",
    "df['PCA2'] = pca_features[:, 1]\n",
    "df['PCA3'] = pca_features[:, 2]\n",
    "\n",
    "# Create a TSNE object\n",
    "tsne = TSNE(n_components=3)\n",
    "# Fit the TSNE and transform the data\n",
    "tsne_features = tsne.fit_transform(df[['CNN', 'AE', 'VGG']])\n",
    "# Add the components to the dataframe\n",
    "df['TSNE1'] = tsne_features[:, 0]\n",
    "df['TSNE2'] = tsne_features[:, 1]\n",
    "df['TSNE3'] = tsne_features[:, 2]\n",
    "\n",
    "# Create a scatter plot of the PCA features\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(data=df, x='PCA1', y='PCA2', hue='Ground Truth', style='Meta Learner', palette='Set2')\n",
    "plt.title('PCA')\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot of the TSNE features\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(data=df, x='TSNE1', y='TSNE2', hue='Ground Truth', style='Meta Learner', palette='Set2')\n",
    "plt.title('TSNE')\n",
    "plt.show()\n",
    "\"\"\"\"\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for the aggregation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"0\", \"1\"])\n",
    "disp.plot(cmap='Blues', colorbar=False)\n",
    "plt.title('Confusion Matrix of the Stacking Generalization')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
